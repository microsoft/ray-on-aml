{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9f9e1f",
   "metadata": {},
   "source": [
    "## Distributed Deep Learning with Ray Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904761a1",
   "metadata": {},
   "source": [
    "### 1. Run distributed DL in interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fb9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment,ScriptRunConfig\n",
    "from ray_on_aml.core import Ray_On_AML\n",
    "import time\n",
    "ws = Workspace.from_config()\n",
    "ray_on_aml =Ray_On_AML(ws=ws, compute_cluster =\"gpunc6\",additional_pip_packages=['torch', 'torchvision==0.8.1'], maxnode=2,exp_name='distributed_dl')\n",
    "\n",
    "ray = ray_on_aml.getRay(gpu_support=True)\n",
    "# Note that by default, ci_is_head=True which means  compute instance as head node and all nodes in the remote compute cluster as workers \n",
    "# But if you want to use one of the nodes in the remote AML compute cluster is used as head node and the remaining are worker nodes.\n",
    "# then simply specify ray = ray_on_aml.getRay(ci_is_head=False)\n",
    "# To install additional library, use additional_pip_packages and additional_conda_packages parameters.\n",
    "time.sleep(50)\n",
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab93606",
   "metadata": {},
   "source": [
    "### Train Multi-GPU Distributed Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5de244",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ray.train.torch\n",
    "from ray import train\n",
    "from ray.train import Trainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.utils import consume_prefix_in_state_dict_if_present\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from ray.train import Trainer, TrainingCallback\n",
    "from typing import List, Dict\n",
    "\n",
    "class PrintingCallback(TrainingCallback):\n",
    "    def handle_result(self, results: List[Dict], **info):\n",
    "#         print(results)\n",
    "        print('hello')\n",
    "def train_func(config):\n",
    "    n = 100\n",
    "    # create a toy dataset\n",
    "    # data   : X - dim = (n, 4)\n",
    "    # target : Y - dim = (n, 1)\n",
    "    X = torch.Tensor(np.random.normal(0, 1, size=(n, 4)))\n",
    "    Y = torch.Tensor(np.random.uniform(0, 1, size=(n, 1)))\n",
    "    # toy neural network : 1-layer\n",
    "    # wrap the model in DDP\n",
    "    model = ray.train.torch.prepare_model(nn.Linear(4, 1))\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=3e-4)\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        y = model.forward(X)\n",
    "        # compute loss\n",
    "        loss = criterion(y, Y)\n",
    "        print(\"epoch \", epoch, \" loss \", loss)\n",
    "\n",
    "        # back-propagate loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # To fetch non-DDP state_dict\n",
    "        # w/o DDP: model.state_dict()\n",
    "        # w/  DDP: model.module.state_dict()\n",
    "        # See: https://github.com/ray-project/ray/issues/20915\n",
    "        state_dict = model.state_dict()\n",
    "#         consume_prefix_in_state_dict_if_present(state_dict, \"module.\")\n",
    "        train.save_checkpoint(epoch=epoch, model_weights=state_dict)\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(backend=\"torch\", num_workers=2)\n",
    "trainer.start()\n",
    "trainer.run(train_func, config={\"num_epochs\": 5}, callbacks=[PrintingCallback()])\n",
    "trainer.shutdown() # clean up resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcdd907",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_on_aml.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e6661",
   "metadata": {},
   "source": [
    "### 2. Run distributed Deep Learning in job mode "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96511d",
   "metadata": {},
   "source": [
    "#### Checkout the distributed_ml.py, the job.yml and the conda.yml files. This setup is for Azure ML CLI v2\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90da940c",
   "metadata": {},
   "source": [
    "run the command to submit the job to your AML environment\n",
    "````az ml job create -f job.yml --resource-group <YOUR_RESOURCE_GROUP> --workspace-name <YOUR_WORKSPACE_NAME>````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f4c8d",
   "metadata": {},
   "source": [
    "You can also run the job using v1 SDK as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf3fa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Environment,ScriptRunConfig\n",
    "# from azureml.widgets import RunDetails\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DockerConfiguration,RunConfiguration\n",
    "\n",
    "#Remember the AML job has to have distribted setings (MPI type) for ray-on-aml to work correctly.\n",
    "ws = Workspace.from_config()\n",
    "compute_cluster = 'gpunc6' #This can be another cluster different from the interactive cluster. \n",
    "ray_cluster = ComputeTarget(workspace=ws, name=compute_cluster)\n",
    "\n",
    "aml_run_config_ml = RunConfiguration(communicator='OpenMpi')\n",
    "docker_config = DockerConfiguration(use_docker=True, shm_size='48gb')\n",
    "\n",
    "\n",
    "rayEnv = Environment.from_conda_specification(name = \"RLEnv\",\n",
    "                                             file_path = \"conda.yml\")\n",
    "rayEnv.docker.base_image = \"mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20220412.v1\"\n",
    "\n",
    "aml_run_config_ml.target = ray_cluster\n",
    "aml_run_config_ml.node_count = 2\n",
    "aml_run_config_ml.environment = rayEnv\n",
    "aml_run_config_ml.docker =docker_config\n",
    "\n",
    "src = ScriptRunConfig(source_directory='.',\n",
    "                    script='distributed_ml.py',\n",
    "                    run_config = aml_run_config_ml,\n",
    "                   )\n",
    "\n",
    "run = Experiment(ws, \"distributed_ml\").submit(src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edbee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run).show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7f364c9551711cd4699acda32e0312c3edab483ae246bf330de758088cecccb"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
